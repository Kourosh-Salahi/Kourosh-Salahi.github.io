<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>CS180 Project 4 — Neural Radiance Fields</title>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;800&display=swap" rel="stylesheet">

  <style>
    :root { --bg:#0b1020; --card:#111836; --muted:#a0aec0; --ink:#e6eefc; --accent:#8ab4ff; }
    body { margin:0; font-family:Inter,system-ui,sans-serif; background:var(--bg); color:var(--ink); line-height:1.6; }
    .container { max-width:1100px; margin:0 auto; padding:20px; }
    header { padding:32px 20px; background:linear-gradient(to bottom,rgba(11,16,32,0.95),rgba(11,16,32,0.7)); border-bottom:1px solid rgba(255,255,255,0.08); position:sticky; top:0; z-index:10; }
    .brand { display:flex; align-items:center; gap:12px; }
    .brand .dot { width:10px; height:10px; border-radius:50%; background:var(--accent); }
    .brand h1 { font-size:18px; margin:0; font-weight:800; }
    nav { margin-left:auto; font-size:14px; display:flex; gap:12px; }
    nav a { color:var(--accent); text-decoration:none; }
    nav a:hover { text-decoration:underline; }
    .card { background:rgba(255,255,255,0.03); border:1px solid rgba(255,255,255,0.08); border-radius:18px; padding:20px; margin-bottom:30px; }
    .hero-title { font-size:clamp(28px,4vw,44px); font-weight:800; margin:0 0 10px; }
    .subtitle { color:var(--muted); margin:0 0 10px; }
    .grid { display:grid; gap:16px; }
    .grid.two { grid-template-columns:repeat(2,1fr); }
    .grid.three { grid-template-columns:repeat(3,1fr); }
    .grid.four { grid-template-columns:repeat(4,1fr); }
    figure { margin:0; cursor:zoom-in; }
    img { width:100%; max-height:250px; object-fit:contain; border-radius:12px; border:1px solid rgba(255,255,255,0.08); }
    figcaption { font-size:14px; color:var(--muted); }
    pre { background:rgba(255,255,255,0.06); border:1px solid rgba(255,255,255,0.08); padding:14px; border-radius:12px; overflow:auto; font-size:13px; white-space:pre-wrap; }
    code { font-family:"Courier New", monospace; color:#9ed0ff; }
    .lightbox { display:none; position:fixed; inset:0; background:rgba(7,10,22,0.9); z-index:1000; justify-content:center; align-items:center; padding:40px; }
    .lightbox img { max-width:90%; max-height:90%; border-radius:18px; box-shadow:0 20px 60px rgba(0,0,0,0.6); cursor:zoom-out; }
  </style>
</head>

<body>
  <header>
    <div class="container brand">
      <span class="dot"></span>
      <h1>CS180 • Project 4 · NeRF</h1>
      <nav>
        <a href="#part0">Part 0</a>
        <a href="#part1">Part 1</a>
        <a href="#part2">Part 2</a>
        <a href="#part26">My NeRF</a>
        <a href="#conclusion">Conclusion</a>
      </nav>
    </div>
  </header>

  <main class="container">

    <!-- HERO -->
    <section class="card">
      <h2 class="hero-title">Neural Radiance Fields</h2>
      <p class="subtitle">By <strong>Kourosh Salahi</strong> · CS180 / 280A</p>
      <p>
        This project implements NeRF completely from scratch: camera calibration,
        pose estimation, ray sampling, neural fields, volume rendering,
        and training NeRFs on both the classic Lego dataset and my own captured data.
      </p>
    </section>

    <!-- PART 0 -->
    <section class="card" id="part0">
      <h2>Part 0 — Camera Calibration & Dataset Capture</h2>

      <h3>Camera Frustums Visualization</h3>
      <div class="grid.two">
        <figure><figcaption>Viser camera view 1</figcaption><img src="media/vis_cam1.png" onclick="openLightbox(this.src)"></figure>
        <figure><figcaption>Viser camera view 2</figcaption><img src="media/vis_cam2.png" onclick="openLightbox(this.src)"></figure>
      </div>

      <h3>Calibration Visualization</h3>
      <figure>        <figcaption>Detected ArUco corners used for calibration</figcaption>

        <img src="media/calibration_vis.png" onclick="openLightbox(this.src)">
      </figure>
    </section>

    <!-- PART 1 -->
    <section class="card" id="part1">
      <h2>Part 1 — Neural Field Fitting (2D)</h2>

      <h3>Model Architecture</h3>
      <p>Below is the exact model used for my best results (width=256, L=10):</p>

      <pre><code>
def train(width, L, fp, k, epochs):

  model = nn.Sequential(
    nn.Linear(4*L + 2, width),
    nn.ReLU(),
    nn.Linear(width, width),
    nn.ReLU(),
    nn.Linear(width, width),
    nn.ReLU(),
    nn.Linear(width, 3),
    nn.Sigmoid()
  )

  optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)
  loss_fn = nn.MSELoss()

  ...
      </code></pre>

      <h3>Training Progression — Fox (Width=256, L=10)</h3>
      <p>Training iterations shown: 0 → 50 → 100 → 200 → 500 → 1000 → 1999.</p>

      <div class="grid.three">
        <figure><img src="media/fox_256_10_iter0.png"></figure>
        <figure><img src="media/fox_256_10_iter50.png"></figure>
        <figure><img src="media/fox_256_10_iter100.png"></figure>

        <figure><img src="media/fox_256_10_iter200.png"></figure>
        <figure><img src="media/fox_256_10_iter500.png"></figure>
        <figure><img src="media/fox_256_10_iter1000.png"></figure>

        <figure><img src="media/fox_256_10_iter1999.png"></figure>
      </div>

      <h3>Training Progression — Monkey (Width=256, L=10)</h3>
      <p>Training iterations shown: 0 → 50 → 100 → 200 → 500 → 1000 → 1999.</p>

      <div class="grid.three">
        <figure><img src="media/monkey_iter0.png"></figure>
        <figure><img src="media/monkey_iter50.png"></figure>
        <figure><img src="media/monkey_iter100.png"></figure>

        <figure><img src="media/monkey_iter200.png"></figure>
        <figure><img src="media/monkey_iter500.png"></figure>
        <figure><img src="media/monkey_iter1000.png"></figure>

        <figure><img src="media/monkey_iter1999.png"></figure>
      </div>

      <h3>Final Results </h3>
      <p>Four combinations of PE frequency (L ∈ {4,10}) and width (W ∈ {64,256}), all at iter1999. Please note all image captions are below each image</p>

    <div class="grid.two">
      <figure><figcaption>W=64, L=4</figcaption><img src="media/fox_64_4_iter1999.png"></figure>
      <figure><figcaption>W=64, L=10</figcaption><img src="media/fox_64_10_iter1999.png"></figure>
      <figure><figcaption>W=256, L=4</figcaption><img src="media/fox_256_4_iter1999.png"></figure>
      <figure><figcaption>W=256, L=10</figcaption><img src="media/fox_256_10_iter1999.png"></figure>
    </div>
    
    <h3>PSNR Curves (Fox)</h3>
    
    <div class="grid.two">
      <figure><figcaption>W=64, L=4</figcaption><img src="media/fox_psnr_64_4.png"></figure>
      <figure><figcaption>W=64, L=10</figcaption><img src="media/fox_psnr_64_10.png"></figure>
      <figure><figcaption>W=256, L=4</figcaption><img src="media/fox_psnr_256_4.png"></figure>
      <figure><figcaption>W=256, L=10</figcaption><img src="media/fox_psnr_256_10.png"></figure>
    </div>

    </section>

    <!-- PART 2 -->
<section class="card" id="part2">
  <h2>Part 2 — Neural Radiance Field on Multi-View Lego</h2>

  <p>
    In this section, I implemented a complete NeRF pipeline: converting image pixels to
    3D world-space rays, sampling points along those rays, evaluating a radiance field
    network, and performing differentiable volume rendering to optimize the model
    against multiple posed RGB images.
  </p>

  <!-- 2.1 Implementation Summary -->
  <h3>2.1 Camera → Ray Pipeline</h3>

  <p>
    <strong>Camera-to-World Transform:</strong>
    I implemented a general <code>transform(c2w, x_c)</code> function that converts
    camera-space coordinates to world-space using homogeneous coordinates.
    I verified correctness by ensuring the round-trip identity
    <code>x == transform(inv(c2w), transform(c2w, x))</code> holds for random test points.
  </p>

  <p>
    <strong>Pixel → Camera Coordinates:</strong>
    Each pixel coordinate is unprojected by inverting the pinhole camera model:
    subtracting the principal point, dividing by focal length, and assigning a unit depth.
    This produces a batch of 3D points in camera coordinates.
  </p>

  <p>
    <strong>Pixel → Ray:</strong>
    For each pixel <code>(u, v)</code>, the ray origin is simply the translation component
    of <code>c2w</code>, and the ray direction is obtained by transforming the camera-space
    point into world space and normalizing:
    <br>
    <code>ray_d = normalize( transform(c2w, x_cam) − origin )</code>.
  </p>


  <!-- 2.2 Sampling -->
  <h3>2.2 Sampling Rays & Points</h3>

  <p>
    I implemented <strong>global random sampling</strong> across all pixels of all training images,
    avoiding an image-by-image sampling loop. This produces highly uniform ray coverage.
    Each sampled ray is discretized into <strong>64 points</strong> between
    <code>near = 2.0</code> and <code>far = 6.0</code>.
    During training, I add uniform jitter within each interval to avoid aliasing and
    improve spatial coverage.
  </p>


  <!-- 2.3 Dataloader -->
  <h3>2.3 Dataloader + Precomputation + Viser Visualizations</h3>

  <p>
    To accelerate training, I built a preprocessing stage that:
  </p>

  <ul>
    <li>enumerates every pixel in every image with a +0.5 center offset,</li>
    <li>stores their RGB values,</li>
    <li>stores a repeated copy of the camera extrinsic for each pixel,</li>
    <li>precomputes world-space <code>ray_o</code> and <code>ray_d</code> for the entire dataset.</li>
  </ul>

  <p>
    The dataloader then samples rays by simply indexing into these large flattened arrays.
    I validated correctness of UV alignment, ray orientation, and frustum consistency
    using Viser visualizations.
  </p>

  <div class="grid.two">
    <figure>
      <figcaption>Rays from one view (sampled)</figcaption>
      <img src="media/Tractor_single.png" onclick="openLightbox(this.src)">
    </figure>
    <figure>
      <figcaption>Different region of same camera</figcaption>
      <img src="media/Tractor_single_2.png" onclick="openLightbox(this.src)">
    </figure>
  </div>

  <div class="grid.two" style="margin-top:15px;">
    <figure>
      <figcaption>Top-left pixel rays</figcaption>
      <img src="media/Tractor_single_3.png" onclick="openLightbox(this.src)">
    </figure>
    <figure>
      <figcaption>All cameras + 100 rays</figcaption>
      <img src="media/Tractor_many views.png" onclick="openLightbox(this.src)">
    </figure>
  </div>

  <div class="grid.one" style="margin-top:15px;">
    <figure>
      <figcaption>Camera cluster with sample points</figcaption>
      <img src="media/Tractor_many_views_2.png" onclick="openLightbox(this.src)">
    </figure>
  </div>


  <h3>2.4 Neural Radiance Field MLP</h3>

<p>
  My NeRF model closely follows the original architecture: a deep point MLP with a
  skip connection, a density head, and a separate view-dependent color head. Both 3D
  sample locations and ray directions use sinusoidal positional encoding before being
  fed into the network.
</p>

<figure style="max-width:500px;margin:0 auto 18px auto;">
  <figcaption style="text-align:center;margin-bottom:6px;">NeRF MLP Diagram</figcaption>
  <img src="media/mlp_nerf.png" onclick="openLightbox(this.src)">
</figure>

<ul>

  <li>
    <strong>Positional Encoding:</strong>
    3D points are encoded using <code>L<sub>x</sub>=10</code>, giving
    <code>3 + 6L = 63</code> channels.  
    Ray directions use <code>L<sub>r</sub>=4</code>, producing
    <code>3 + 6L = 27</code> channels.
  </li>

  <li>
    <strong>Point MLP (σ + features):</strong>
    The encoded 3D point is processed by an 8-layer MLP with width 256.  
    At the 4th hidden layer (index 3), I concatenate the original positional encoding
    back into the network, exactly matching:
    <br>
    <code>Linear(256 + input_dim_x → 256)</code>.  
    This skip connection helps preserve high-frequency spatial detail.
  </li>

  <li>
    <strong>Density Head (σ):</strong>
    After the final point-layer, a <code>256 → 1</code> linear projection followed by ReLU
    produces the volume density <code>σ</code>.
  </li>

  <li>
    <strong>Color Head (rgb):</strong>
    The point-MLP output is first transformed into a 256-D feature vector.  
    This feature is concatenated with the encoded ray direction (27-D) and passed through:
    <ul>
      <li><code>(256 + input_dim_r) → 128</code> with ReLU,</li>
      <li><code>128 → 3</code> with Sigmoid to produce RGB in <code>[0,1]</code>.</li>
    </ul>
  </li>

  <li>
    <strong>Outputs:</strong>
    The network returns <code>σ</code> (density) and <code>rgb</code> (color).
  </li>

</ul>

<!-- 2.5 Volume Rendering -->
<h3>2.5 Volume Rendering</h3>

<p>
  I implemented NeRF’s differentiable volume rendering entirely in a vectorized
  form. For each ray, the sampled densities <code>σ</code> and colors <code>rgb</code>
  are combined using the standard NeRF weighting:
</p>

<pre><code>w_i = T_i · (1 − exp(−σ_i Δt))</code></pre>

<p>
  where the transmittance <code>T_i</code> is computed using a shifted cumulative sum of
  the densities:
</p>

<pre><code>T_i = exp( − cumulative_sum(σ · Δt, shifted) )</code></pre>

<p>
  This matches the continuous volume rendering equation while remaining efficient,
  since all rays and samples are processed in parallel. The final pixel color is:
</p>

<pre><code>C = Σ w_i · rgb_i</code></pre>

<p>
  This implementation corresponds exactly to my <code>volrend()</code> function, using
  fully vectorized PyTorch operations without loops.
</p>


  <!-- 2.6 Training Progression -->
  <h3>Training Progression (0 → 1400 iterations)</h3>

  <p>
    Below are renders from the validation camera at key training iterations:
    <strong>0, 200, 400, 600, 800, 1000, 1200, 1400</strong>.
  </p>

  <div class="grid.four">
    <figure><figcaption>iter 0</figcaption><img src="media/tractor_training1.png"></figure>
    <figure><figcaption>iter 200</figcaption><img src="media/tractor_training2.png"></figure>
    <figure><figcaption>iter 400</figcaption><img src="media/tractor_training3.png"></figure>
    <figure><figcaption>iter 600</figcaption><img src="media/tractor_training4.png"></figure>

    <figure><figcaption>iter 800</figcaption><img src="media/tractor_training5.png"></figure>
    <figure><figcaption>iter 1000</figcaption><img src="media/tractor_training6.png"></figure>
    <figure><figcaption>iter 1200</figcaption><img src="media/tractor_training7.png"></figure>
    <figure><figcaption>iter 1400</figcaption><img src="media/tractor_training8.png"></figure>
  </div>


  <!-- PSNR -->
  <h3>PSNR Curves</h3>

  <p>
    I tracked PSNR on both the training set and the held-out validation set:
  </p>

  <div class="grid.two">
    <figure>
      <figcaption>Training PSNR</figcaption>
      <img src="media/Tractor_PSNR.png" onclick="openLightbox(this.src)">
    </figure>
    <figure>
      <figcaption>Validation PSNR</figcaption>
      <img src="media/Tractor_PSNR_Validation.png" onclick="openLightbox(this.src)">
    </figure>
  </div>


  <!-- Spherical Render -->
  <h3>Spherical Rendering (Novel Views)</h3>

  <figure>
    <figcaption>Rendered novel-view orbit of Lego</figcaption>
    <img src="media/Tractor_nerf_rotation.gif" onclick="openLightbox(this.src)">
  </figure>

</section>

</section>

<!-- PART 2.6 -->
<section class="card" id="part26">
  <h2>Part 2.6 — NeRF Trained on My Own Captured Object</h2>

  <p>
    For this section, I trained a full Neural Radiance Field using the dataset
    captured in Part&nbsp;0. This involved using my own calibrated images and
    camera poses, then training a NeRF model identical in structure to Part 2,
    with adjustments to near/far ranges and sampling strategy due to the much
    smaller physical scale of the scene.
  </p>

  <h3>Model Architecture & Training Setup</h3>

  <p>
    I used a custom NeRF architecture with skip connection, separate density/color
    branches, and independent positional encoding frequencies for 3D sample points
    and view directions.
  </p>

  <h3>Hyperparameter & Implementation Adjustments</h3>
  <ul>
    <li><strong>near = 0.2</strong> and <strong>far = 0.5</strong>  
      (real-world scene was much smaller than Lego dataset)</li>
    <li><strong>64 samples per ray</strong>, jittered during training</li>
    <li><strong>Positional encoding:</strong> L<sub>x</sub> = 10 for positions, L<sub>r</sub> = 4 for directions</li>
    <li><strong>Adam optimizer:</strong> lr = 5e-4</li>
  </ul>

  <p>
    These adjustments were essential to avoid the model sampling empty space and to
    keep rays bounded within the tight capture volume around the object.
  </p>

  <h3>Training Loss Over Time</h3>

  <div class="grid.two">
    <figure>      <figcaption>MSE Loss Curve</figcaption>

      <img src="media/car_mse.png" onclick="openLightbox(this.src)">
    </figure>
    <figure>      <figcaption>PSNR Over Training</figcaption>

      <img src="media/car_psnr.png" onclick="openLightbox(this.src)">
    </figure>
  </div>

<!-- PART 2.6 — Intermediate Renders -->
<h3>Intermediate Training Renders</h3>
<p>
  Below are snapshots of the NeRF’s output at various stages of training.
  These demonstrate how density and color predictions refine over time.
</p>

<div class="grid.five">
  <figure><figcaption>iter 0</figcaption><img src="media/car_iter0.png"></figure>
  <figure><figcaption>iter 200</figcaption><img src="media/car_iter200.png"></figure>
  <figure><figcaption>iter 400</figcaption><img src="media/car_iter400.png"></figure>
  <figure><figcaption>iter 600</figcaption><img src="media/car_iter600.png"></figure>
  <figure><figcaption>iter 800</figcaption><img src="media/car_iter800.png"></figure>

  <figure><figcaption>iter 1000</figcaption><img src="media/car_iter1000.png"></figure>
  <figure><figcaption>iter 1200</figcaption><img src="media/car_iter1200.png"></figure>
  <figure><figcaption>iter 1400</figcaption><img src="media/car_iter1400.png"></figure>
  <figure><figcaption>iter 1600</figcaption><img src="media/car_iter1600.png"></figure>
  <figure><figcaption>iter 1800</figcaption><img src="media/car_iter1800.png"></figure>
</div>


<h3>Reference Image</h3>
<figure>  <figcaption>Ground Truth Reference</figcaption>

  <img src="media/car_iterref.png">
</figure>


  <h3>Novel-View Rendering (Final)</h3>
  <p>
    After training, I generated a 360° orbit animation by using the provided
    “look-at-origin” camera generation code, producing a smooth camera path around
    the object. Because my Aruco tag was so big, I pointed it towards a different corner rather than the origin corner. Thus, I had to change the logic for the roation function to rotate along a different world point.
    The final rendered GIF is shown below:
  </p>

  <figure>    <figcaption>Novel-view orbit of my reconstructed NeRF</figcaption>

    <img src="media/MY_nerf_rotation2.gif" onclick="openLightbox(this.src)">
  </figure>

  <p class="subtitle">
    These results demonstrate that my custom-captured dataset can successfully train
    a NeRF despite real-world noise, lens distortion, uneven lighting, and limited
    viewpoints.
  </p>
</section>


    <!-- Conclusion -->
    <section class="card" id="conclusion">
      <h2>Conclusion</h2>
      <p>
        I implemented a complete Neural Radiance Field pipeline from scratch.  
        From camera calibration and PnP, to neural fields, ray sampling,  
        and full volumetric rendering, this project deepened my understanding  
        of differentiable rendering and scene representation.
      </p>
    </section>

  </main>

  <!-- LIGHTBOX -->
  <div id="lightbox" class="lightbox" onclick="closeLightbox()">
    <img id="lightbox-img" src="">
  </div>

  <script>
    function openLightbox(src) {
      document.getElementById('lightbox-img').src = src;
      document.getElementById('lightbox').style.display = 'flex';
    }
    function closeLightbox() {
      document.getElementById('lightbox').style.display = 'none';
    }
  </script>

</body>
</html>
